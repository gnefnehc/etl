{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 輸入版名後，將該版文章全部爬下來"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests as r\n",
    "from bs4 import BeautifulSoup\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 爬取該頁 每篇文章的網址"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def list_crawler(url):\n",
    "    \"\"\" This is the function for crawling urls\n",
    "        arg = \"urls\"\n",
    "        return = url list\n",
    "    \"\"\"\n",
    "    links = []\n",
    "    res = r.get(url)\n",
    "    soup = BeautifulSoup(res.text, 'lxml')\n",
    "    index_links = soup.find(class_=\"r-list-container\").select('a')\n",
    "    for link in index_links:\n",
    "        links.append(\"http://www.ptt.cc\" + link['href'])\n",
    "    return links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 讀取最新一頁的網址"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_first_nextpage(url):\n",
    "    \"\"\" This is the function to get first next page url\n",
    "        arg = \"url\"\n",
    "        return = first next page's url\n",
    "    \"\"\"\n",
    "    first_index = \"\"\n",
    "    res = r.get(url)\n",
    "    soup = BeautifulSoup(res.text, 'lxml')\n",
    "    first_index = (soup.find(class_=\"btn-group-paging\").find_all(class_=\"btn\")[1]['href'])\n",
    "    first = int(first_index.split('/')[3].split('.')[0].replace(\"index\", \"\"))\n",
    "    first_page = \"index\" + str(first + 1)\n",
    "    return first_page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 爬取每篇文章內文 包含 作者 標題 時間 內文，目前還沒有爬推文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def content_crawler(url):\n",
    "    \"\"\" This is the function to get content from the list url\n",
    "        arg = \"url\"\n",
    "        return = content{\n",
    "                            author = \"\"\n",
    "                            title = \"\"\n",
    "                            datetime = \"\"\n",
    "                            content = \"\"    }        \n",
    "    \"\"\"\n",
    "    content = {}\n",
    "    res = r.get(url)\n",
    "    soup = BeautifulSoup(res.text, 'lxml')\n",
    "    content = {\n",
    "    'author' : soup.find(id=\"main-container\").find_all('span')[1].text,\n",
    "    'title' : soup.find(id=\"main-container\").find_all('span')[5].text,\n",
    "    'datetime' : soup.find(id=\"main-container\").find_all('span')[7].text,\n",
    "    'content' : content_inner_text(soup)}\n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 爬內文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def content_inner_text(soup):\n",
    "    \"\"\" This is the function to get content body\n",
    "        arg = soup\n",
    "        return = \"content body\"\n",
    "    \"\"\"\n",
    "    body = \"\"\n",
    "    soup1 = soup.find(id=\"main-container\")\n",
    "    [s.extract() for s in soup1('span')]\n",
    "    body = soup1.text.split('--')[0]\n",
    "    return body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 輸入PTT 版名\n",
    "### 從最新一頁開始，爬每一篇文章\n",
    "### 會從頭爬到尾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ptt_crawler(name):\n",
    "    url = \"https://www.ptt.cc/bbs/\" + name + \"/index.html\"\n",
    "    \"\"\" This is the function to crawler a PTT board\n",
    "        arg = ptt board name\n",
    "        save each page to a json file (max: 20 articles)\n",
    "    \"\"\"\n",
    "    # get the latest page number and change it into int i for counting\n",
    "    first_page = get_first_nextpage(url)\n",
    "    i = int(first_page.replace(\"index\", \"\"))\n",
    "    \n",
    "#    while ( i >= 0):\n",
    "    while ( i >= 925): # if you don't want to crawl all the page set cutoff here\n",
    "        \n",
    "        # get pageList\n",
    "        page_list = list_crawler(\"https://www.ptt.cc/bbs/PokeMon/index\" + str(i) + \".html\")\n",
    "        i -= 1\n",
    "        \n",
    "        linkcnt = 0\n",
    "#        print page_list \n",
    "        print i # check which page we've crawl\n",
    "        \n",
    "        for links in page_list:\n",
    "\n",
    "            temp = content_crawler(links)\n",
    "            temp_content = json.dumps(temp, ensure_ascii=False)\n",
    "            linkcnt += 1\n",
    "            # set the path for saving json file\n",
    "            with open('E:/ETL/OutPut/PokeMon/' + str(i) +'.json', 'a') as f:\n",
    "                prefix = \"\"\n",
    "                append_content = \"\"\n",
    "                postfix = \"\"\n",
    "                if linkcnt == 1:\n",
    "                    prefix = \"{\"\n",
    "\n",
    "                if len(page_list) > 1 and linkcnt < len(page_list):\n",
    "                    append_content = \"\\\"\" + links + \"\\\"\" + \":\" + temp_content.encode('utf-8') + \",\"\n",
    "                elif linkcnt == len(page_list):\n",
    "                    append_content = \"\\\"\" + links + \"\\\"\" + \":\" + temp_content.encode('utf-8')\n",
    "\n",
    "                if linkcnt == len(page_list):\n",
    "                    postfix = \"}\"\n",
    "\n",
    "                f.write(prefix + append_content + postfix)\n",
    "\n",
    "        print len(page_list) # check how many articles we've crawl for each json file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 就決定是你了 神奇寶貝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ptt_crawler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c229e3939ba6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mptt_crawler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"PokeMon\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'ptt_crawler' is not defined"
     ]
    }
   ],
   "source": [
    "ptt_crawler(\"PokeMon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
