{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests as r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = r.get(\"http://www.appledaily.com.tw/realtimenews/section/new/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check the type of res\n",
    "type(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use dir() to check the methods and attributes of Response Object\n",
    "dir(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# POST\n",
    "data = {\n",
    "    'key': 'val'\n",
    "}\n",
    "res = r.post(\"https://httpbin.org/post\", data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print res.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check the response header\n",
    "# Note that the Content-Type is 'application/json'\n",
    "res.headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use Response.json() method to decode JSON string into python Dictionary\n",
    "res.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BeautifulSoup(HTML parsing)\n",
    "# text/HTML -> Python objects\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = r.get(\"http://www.appledaily.com.tw/realtimenews/section/new/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(res.text, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "soup.find_all(class_='rtddt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "news_list = soup.find_all('li', class_='rtddt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for news in news_list:\n",
    "    print news.font.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Appledaily complete example\n",
    "# STEP1: Find url pattern\n",
    "# http://www.appledaily.com.tw/realtimenews/section/new/1 => page 1\n",
    "# http://www.appledaily.com.tw/realtimenews/section/new/2 => page 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "one_to_ten = range(1,11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(one_to_ten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generator or iterator\n",
    "xrange(1,11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(xrange(1,11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's crawl 10 pages of Appledaily\n",
    "for page in range(1,11):\n",
    "    print \"Crawling page {}\".format(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's crawl 10 pages of Appledaily\n",
    "URL = \"http://www.appledaily.com.tw/realtimenews/section/new/\"\n",
    "for page in range(1,11):\n",
    "    # page is type int\n",
    "    print \"Crawling page {}\".format(page)\n",
    "    #current_url = URL + page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def list_crawler(url):\n",
    "    \"\"\" This is a function that crawls the list page of appledaily news\n",
    "        Args:\n",
    "             url: The url of news list page\n",
    "        Return:\n",
    "             article_url_list: list of article urls\n",
    "    \"\"\"\n",
    "    article_url_list = []\n",
    "    res = r.get(url)\n",
    "    soup = BeautifulSoup(res.text, 'lxml')\n",
    "    news_list = soup.find_all('li', class_='rtddt')\n",
    "    for news in news_list:\n",
    "        print news.font.text\n",
    "        # news.a['href'] does not include host http://www.appledaily.com.tw\n",
    "        print \"http://www.appledaily.com.tw\" + news.a['href']\n",
    "        article_url_list.append(\"http://www.appledaily.com.tw\" + news.a['href'])\n",
    "    return article_url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "url_list = list_crawler(\"http://www.appledaily.com.tw/realtimenews/section/new/1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for url in url_list:\n",
    "    print url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_article(url):\n",
    "    \"\"\"This function extracts the title and content of the news article\n",
    "       Args:\n",
    "            url: url of the article\n",
    "       Return:\n",
    "            art_dict: dictionary of artilce title and content\n",
    "            \n",
    "            art_dict = {\n",
    "                'title': \"I am a title\",\n",
    "                'content': \"This is the content of the news artile\",\n",
    "                'pub_date': \"2016年08月26日09:15\",\n",
    "                'category': \"sports\"\n",
    "            }\n",
    "    \n",
    "    \"\"\"\n",
    "    art_dict = {}\n",
    "    res = r.get(url)\n",
    "    soup = BeautifulSoup(res.text, 'lxml')\n",
    "    art_dict['title'] = soup.find(id='h1').text\n",
    "    art_dict['content'] = soup.find(id='summary').text\n",
    "    art_dict['pub_date'] = soup.find('time').text\n",
    "#     print url\n",
    "#     print url.split('/')\n",
    "#     print url.split('/')[5]\n",
    "    art_dict['category'] = url.split('/')[5]\n",
    "    return art_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "article_dict = parse_article(\"http://www.appledaily.com.tw/realtimenews/article/sports/20160826/936183/%E7%B6%93%E5%85%B8%E8%B3%BD%E5%85%A8%E8%B3%BD%E7%A8%8B%E5%87%BA%E7%88%90%E3%80%80%E6%9C%80%E7%B5%82%E6%B1%BA%E6%88%B0%E9%81%93%E5%A5%87%E7%90%83%E5%A0%B4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print article_dict\n",
    "print article_dict['title']\n",
    "print article_dict['content']\n",
    "print article_dict['pub_date']\n",
    "print article_dict['category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling page 1\n",
      "Current url is http://www.appledaily.com.tw/realtimenews/section/new/1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'list_crawler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c0504014d517>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mcurrent_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mURL\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[1;34m\"Current url is {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0marticle_url_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist_crawler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_url\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# We get the list of article_urls\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0marticle_url\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marticle_url_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'list_crawler' is not defined"
     ]
    }
   ],
   "source": [
    "# let's crawl 10 pages of Appledaily\n",
    "URL = \"http://www.appledaily.com.tw/realtimenews/section/new/\"\n",
    "final_list = []\n",
    "for page in range(1,11):\n",
    "    # page is type int\n",
    "    print \"Crawling page {}\".format(page)\n",
    "    current_url = URL + str(page)\n",
    "    print \"Current url is {}\".format(current_url)\n",
    "    article_url_list = list_crawler(current_url) # We get the list of article_urls\n",
    "    for article_url in article_url_list:\n",
    "        try:\n",
    "            final_list.append(parse_article(article_url))\n",
    "        except Exception as e:\n",
    "            print e\n",
    "print final_list\n",
    "\"\"\"\n",
    "[\n",
    "{'title': 'one', 'content': 'asdfasdfasd', 'pub_date': 'asdfasdfdsf'}, # from parse_article(url)\n",
    "{'title': 'one', 'content': 'asdfasdfasd', 'pub_date': 'asdfasdfdsf'},\n",
    "{'title': 'one', 'content': 'asdfasdfasd', 'pub_date': 'asdfasdfdsf'},\n",
    "{'title': 'one', 'content': 'asdfasdfasd', 'pub_date': 'asdfasdfdsf'},\n",
    "{'title': 'one', 'content': 'asdfasdfasd', 'pub_date': 'asdfasdfdsf'},\n",
    "]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Python 檔案操作\n",
    "# open(file, mode)\n",
    "#open('./myfile.txt', 'r')\n",
    "#open('./myfile.txt', 'w')\n",
    "#open('./myfile.txt', 'a')\n",
    "#open('./myimg.jpg', 'rb')\n",
    "#open('./myimg.jpg', 'wb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('./myfile.txt', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__doc__',\n",
       " '__enter__',\n",
       " '__exit__',\n",
       " '__format__',\n",
       " '__getattribute__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__iter__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " 'close',\n",
       " 'closed',\n",
       " 'encoding',\n",
       " 'errors',\n",
       " 'fileno',\n",
       " 'flush',\n",
       " 'isatty',\n",
       " 'mode',\n",
       " 'name',\n",
       " 'newlines',\n",
       " 'next',\n",
       " 'read',\n",
       " 'readinto',\n",
       " 'readline',\n",
       " 'readlines',\n",
       " 'seek',\n",
       " 'softspace',\n",
       " 'tell',\n",
       " 'truncate',\n",
       " 'write',\n",
       " 'writelines',\n",
       " 'xreadlines']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f.write(\"Hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('./myfile2.txt', 'w')\n",
    "f.write(\"second time\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Best way to do file operations....\n",
    "# \"with\" will close the file for you...\n",
    "with open('./applydaily.json', 'w') as f:\n",
    "    f.write(\"appledaily\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Back to out Appledaily crawler....\n",
    "# Let's write the result to a json file....\n",
    "import json\n",
    "len(final_list)\n",
    "#dir(json)\n",
    "# NOTE: when dealing with json.dumps() and Unicode, make sure to set ensure_ascii=False\n",
    "mystr = json.dumps(final_list, ensure_ascii=False)\n",
    "with open('./applydaily.json', 'w') as f:\n",
    "    f.write(mystr.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      \n",
      "HI\t\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"'      \\\\nHI\\\\t'\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "withspace = '      \\nHI\\t'\n",
    "print withspace\n",
    "repr(withspace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'str'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"'\\\\xe7\\\\x9a\\\\xae\\\\xe5\\\\x8d\\\\xa1\\\\xe4\\\\xb8\\\\x98'\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pokemon = '皮卡丘'\n",
    "print type(pokemon)\n",
    "repr(pokemon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'unicode'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"u'\\\\u76ae\\\\u5361\\\\u4e18'\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pokemon = u'皮卡丘'\n",
    "print type(pokemon)\n",
    "repr(pokemon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# x = 1\n",
    "# while True:\n",
    "#     print x\n",
    "#     x = x + 1\n",
    "#     res = r.get(\"\")\n",
    "#     if res.status_code != '200':\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Selenium\n",
    "---\n",
    "1. pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "driver = webdriver.Firefox()\n",
    "driver.get(\"http://www.python.org\")\n",
    "elem = driver.find_element_by_name(\"q\")\n",
    "elem.clear()\n",
    "elem.send_keys(\"selenium\")\n",
    "elem.send_keys(Keys.RETURN)\n",
    "html_source = driver.page_source\n",
    "print html_source\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "driver = webdriver.Chrome(\"./Downloads/chromedriver.exe\")\n",
    "driver.get(\"https://www.agoda.com/zh-tw/\")\n",
    "driver.find_element_by_xpath(\"//input[@type='text']\").click()\n",
    "driver.find_element_by_xpath(\"//input[@type='text']\").clear()\n",
    "driver.find_element_by_xpath(\"//input[@type='text']\").send_keys(u\"舊金山\")\n",
    "driver.find_element_by_xpath('//*[@id=\"oneline-searchbox\"]/dl[5]/dd/button').click()\n",
    "\n",
    "# Wait ultil some element with class name \"hotel-name\" appear on the page\n",
    "elem_present = expected_conditions.presence_of_element_located((By.CLASS_NAME, 'hotel-name'))\n",
    "WebDriverWait(driver, 10).until(elem_present)\n",
    "\n",
    "for x in range(0,9):\n",
    "    elem = driver.find_element_by_id(\"paginationNext\")\n",
    "    driver.execute_script(\"return arguments[0].scrollIntoView();\", elem)\n",
    "    elem.click()\n",
    "    \n",
    "\n",
    "print driver.page_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "res = requests.get(\"https://www.dcard.tw/_api/posts?popular=false&before=224627985\")\n",
    "res.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# natgeo\n",
    "import requests as r\n",
    "res = r.get(\"http://news.nationalgeographic.com/bin/services/news/public/query/content.json?pageSize=5&page=0&contentTypes=news/components/pagetypes/story/article,news/components/pagetypes/story/gallery\")\n",
    "res.json()\n",
    "len(res.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# natgeo\n",
    "# 100 pages\n",
    "import requests as r\n",
    "res = r.get(\"http://news.nationalgeographic.com/bin/services/news/public/query/content.json?pageSize=100&page=0&contentTypes=news/components/pagetypes/story/article,news/components/pagetypes/story/gallery\")\n",
    "res.json()\n",
    "len(res.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular Expression (正規表達式)\n",
    "---\n",
    "- a-z A-Z 0-9: 一對一match\n",
    "- . : 任意字元除\\n\n",
    "- \\w : a-z A-Z 0-9\n",
    "- \\d : 0-9\n",
    "- ^ : start\n",
    "- $ : end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mystr = \"I love pikachu, he is really cute\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "match = re.search(r'pikachu', mystr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pikachu\n"
     ]
    }
   ],
   "source": [
    "print match.group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# + -> repeat previous char 1 to n times\n",
    "# * -> repeat previous char 0 to n times\n",
    "# ? -> 0 or 1 time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h\n"
     ]
    }
   ],
   "source": [
    "mypet = 'dddddd gdddd'\n",
    "myurl = \"https://google.com/\"\n",
    "match = re.search(r'\\w', myurl)\n",
    "#match = re.search(r'\\w+', mypet)\n",
    "#match = re.search(r'\\w+ g?', mypet)\n",
    "print match.group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myemail = 'asdfjasdi asas  39402jsaf ianchen06@gmail.com.tw nice nice pika pika'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "match = re.search(r'\\w+@[\\w.]+', myemail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ianchen06@gmail.com.tw'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match.group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "a = '他欠我 250,000元, 好熱阿'\n",
    "b = 'a欠我 4,000元, 好熱阿'\n",
    "c = 'B欠我 250元, 好熱阿'\n",
    "\n",
    "# [] OR\n",
    "m = re.search('[0123456789]', b)\n",
    "m = re.search('[0-9]', b)\n",
    "m = re.search('\\d', b)\n",
    "\n",
    "print m.group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m = re.search('a', b)\n",
    "#m = re.search('[abcdefghijklmnopqrstuvwxyz]', b)\n",
    "#m = re.search('[a-z]', b)\n",
    "#m = re.search('[a-zA-Z]', c)\n",
    "#m = re.search('[a-zA-Z0-9]', c)\n",
    "#m = re.search('\\w', c)\n",
    "print m.group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = '他欠我 250,000元, 好熱阿'\n",
    "b = 'a欠我 4,000元, 好熱阿'\n",
    "c = 'B欠我 250元, 好熱阿'\n",
    "\n",
    "# {n} repeat n times\n",
    "m = re.search('\\d{3}', c)\n",
    "m = re.search('\\d\\d\\d', c)\n",
    "\n",
    "print m.group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m = re.search('\\d{1,3}', c)\n",
    "m = re.search('\\d{1,3}', b)\n",
    "\n",
    "print m.group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m = re.search('\\d{1,}', c)\n",
    "print m.group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m = re.search('\\d+', c)\n",
    "print m.group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = '他欠我 250,000元, 好熱阿'\n",
    "b = 'a欠我 4,000元, 好熱阿'\n",
    "c = 'B欠我 250元, 好熱阿'\n",
    "\n",
    "print re.search('[0-9,]+', a).group()\n",
    "print re.search('[0-9,]+', b).group()\n",
    "print re.search('[0-9,]+', c).group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Phone\n",
    "phones = ['0912321321', '0932-222-222', '0923-555555'] #, '567876567875465432342','+886932234565']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for phone in phones:\n",
    "    #print phone\n",
    "    m = re.search('09\\d+', phone)\n",
    "    print m.group()\n",
    "print \"=\" * 50\n",
    "for phone in phones:\n",
    "    m = re.search('09\\d{2}-?\\d{3}-?\\d{3}', phone)\n",
    "    print m.group()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#myphone = '+886932234565'\n",
    "phones = ['0912321321', '0932-222-222', '0923-555555', '+886932234565']\n",
    "for myphone in phones:\n",
    "    print re.search('(\\+886)?\\d+-?\\d+-?\\d+', myphone).group(0)\n",
    "    print re.search('(\\+886)?\\d+-?\\d+-?\\d+', myphone).group(1)\n",
    "    print \"=\" * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Basic string manipulation\n",
    "mystr = '123,123,123,123'\n",
    "mystr.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "?mystr.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ''.join(some_list)\n",
    "abc = ['a', 'b', 'c']\n",
    "''.join(abc) # abc[0] + abc[1] + abc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "''.join(mystr.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# datetime module in Python\n",
    "# datetime.datetime.strf\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bday = datetime.strptime('1991-08-12', '%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print bday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(bday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "age1 = bday + timedelta(days=365)\n",
    "age1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#help(timedelta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "age1.strftime('%d-%m-%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(age1.strftime('%d-%m-%Y'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
